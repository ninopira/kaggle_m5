{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:29:34.201240Z",
     "start_time": "2020-05-06T14:29:34.194680Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "# from wrmsse import bild_WRMSSEEvaluator, WRMSSEEvaluator_learge\n",
    "from reduce_mem import reduce_mem_usage\n",
    "from wrmse import weight_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データは従来のまま使いたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:29:35.141508Z",
     "start_time": "2020-05-06T14:29:35.134513Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/new_set_seed/baseline_shop_no_price_again_add_4weekdays_stat_std_shop_cumsum_zerodem_dem_shop_std_no_roll/\n"
     ]
    }
   ],
   "source": [
    "result_dir = './result/new_set_seed/baseline_shop_no_price_again_add_4weekdays_stat_std_shop_cumsum_zerodem_dem_shop_std_no_roll/'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "print(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:34:58.658789Z",
     "start_time": "2020-05-06T14:29:35.642690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "read_transfomed_data\n",
      "Mem. usage decreased to 1719.53 Mb (0.0% reduction)\n",
      "(32782835, 18)\n",
      "(32782835, 18)\n",
      "read_transfomed_data:13.355867624282837[sec]\n",
      "########################\n",
      "########################\n",
      "merge_features...\n",
      "before_merged_shape:(32782835, 18)\n",
      "./feature/shop/f_devine_ave_lag28demand_day_store_dept_no_roll.pkl\n",
      "Mem. usage decreased to 875.40 Mb (30.0% reduction)\n",
      "feature_shape:(32782835, 4)\n",
      "merged_shape:(32782835, 20)\n",
      "merged_time:28.598718643188477[sec]\n",
      "./feature/shop/f_diff_ave_lag28demand_day_store_dept_no_rolling.pkl\n",
      "Mem. usage decreased to 875.40 Mb (30.0% reduction)\n",
      "feature_shape:(32782835, 4)\n",
      "merged_shape:(32782835, 22)\n",
      "merged_time:25.90377974510193[sec]\n",
      "./feature/zero_demand/f_id_zero_demand.pkl\n",
      "Mem. usage decreased to 1062.98 Mb (46.9% reduction)\n",
      "feature_shape:(32782835, 7)\n",
      "merged_shape:(32782835, 27)\n",
      "merged_time:30.7444269657135[sec]\n",
      "./feature/cumsum/f_id_cumsum_demand.pkl\n",
      "Mem. usage decreased to 1250.57 Mb (28.6% reduction)\n",
      "feature_shape:(32782835, 6)\n",
      "merged_shape:(32782835, 31)\n",
      "merged_time:28.9046733379364[sec]\n",
      "./feature/shop/f_diff_ave_sales_day_store_dept_std.pkl\n",
      "Mem. usage decreased to 937.92 Mb (0.0% reduction)\n",
      "feature_shape:(32782835, 5)\n",
      "merged_shape:(32782835, 34)\n",
      "merged_time:27.709707021713257[sec]\n",
      "./feature/lag_demand/f_id_lag_demand_4weekdays_stat.pkl\n",
      "Mem. usage decreased to 1250.57 Mb (54.5% reduction)\n",
      "feature_shape:(32782835, 10)\n",
      "merged_shape:(32782835, 42)\n",
      "merged_time:36.637513875961304[sec]\n",
      "./feature/shop/f_diff_ave_sales_day_store_dept.pkl\n",
      "Mem. usage decreased to 937.92 Mb (0.0% reduction)\n",
      "feature_shape:(32782835, 5)\n",
      "merged_shape:(32782835, 45)\n",
      "merged_time:30.045857429504395[sec]\n",
      "./feature/lag_demand/f_id_lag_demand.pkl\n",
      "Mem. usage decreased to 1688.26 Mb (62.5% reduction)\n",
      "feature_shape:(32782835, 17)\n",
      "merged_shape:(32782835, 60)\n",
      "merged_time:54.529292583465576[sec]\n",
      "./feature/lag_sales/f_id_lag_sales.pkl\n",
      "Mem. usage decreased to 1000.45 Mb (0.0% reduction)\n",
      "feature_shape:(32782835, 6)\n",
      "merged_shape:(32782835, 64)\n",
      "merged_time:30.576702117919922[sec]\n",
      "all_merge_done\n",
      "all_merged_time:293.65122747421265[sec]\n",
      "########################\n",
      "########################\n",
      "date_features...\n",
      "before_date_shape:(32782835, 64)\n",
      "year\n",
      "month\n",
      "dayofweek\n",
      "is_year_end\n",
      "is_year_start\n",
      "add_date_shape:(32782835, 70)\n",
      "date_feature:46.55713415145874[sec]\n",
      "########################\n",
      "df_all_col\n",
      "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'demand',\n",
      "       'part', 'date', 'wm_yr_wk', 'event_name_1', 'event_type_1',\n",
      "       'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI',\n",
      "       'sell_price', 'day_store_dept_sales_std_lag28_demand',\n",
      "       'devine_demand_day_store_dept_lag28',\n",
      "       'day_store_dept_sales_mean_lag28_demand',\n",
      "       'diff_ave_demand_day_store_dept_lag28', '0demand_lag_28_roll_sum_7',\n",
      "       '0demand_lag_28_roll_sum_30', '0demand_lag_28_roll_sum_60',\n",
      "       '0demand_lag_28_roll_sum_90', '0demand_lag_28_roll_sum_180',\n",
      "       'demand_cumsum_lag_28', 'demand_cumsum_lag_90', 'demand_cumsum_lag_180',\n",
      "       'demand_cumsum_lag_364', 'day_store_dept_sales_std_price',\n",
      "       'deviation_sales_day_store_dept', 'lag_deviation_sales_day_store_dept',\n",
      "       'demand_lag_28_x', 'demand_lag_35', 'demand_lag_42', 'demand_lag_49',\n",
      "       '4weekdays_mean', '4weekdays_std', '4weekdays_min', '4weekdays_max',\n",
      "       'day_store_dept_sales_mean_price', 'diff_ave_sales_day_store_dept',\n",
      "       'diff_lag_ave_sales_day_store_dept1', 'demand_lag_28_y',\n",
      "       'demand_lag_29', 'demand_lag_30', 'demand_lag_28_roll_std_7',\n",
      "       'demand_lag_28_roll_std_30', 'demand_lag_28_roll_std_60',\n",
      "       'demand_lag_28_roll_std_90', 'demand_lag_28_roll_std_180',\n",
      "       'demand_lag_28_roll_mean_7', 'demand_lag_28_roll_mean_30',\n",
      "       'demand_lag_28_roll_mean_60', 'demand_lag_28_roll_mean_90',\n",
      "       'demand_lag_28_roll_mean_180', 'demand_lag_28_roll_skew_30',\n",
      "       'demand_lag_28_roll_kurt_30', 'price_change_t1', 'price_change_t365',\n",
      "       'price_rolling_std_t7', 'price_rolling_std_t30', 'year', 'month',\n",
      "       'dayofweek', 'is_year_end', 'is_year_start', 'is_weekend'],\n",
      "      dtype='object')\n",
      "len_x_features:65\n",
      "['0demand_lag_28_roll_sum_180', '0demand_lag_28_roll_sum_30', '0demand_lag_28_roll_sum_60', '0demand_lag_28_roll_sum_7', '0demand_lag_28_roll_sum_90', '4weekdays_max', '4weekdays_mean', '4weekdays_min', '4weekdays_std', 'cat_id', 'day_store_dept_sales_mean_lag28_demand', 'day_store_dept_sales_mean_price', 'day_store_dept_sales_std_lag28_demand', 'day_store_dept_sales_std_price', 'dayofweek', 'demand_cumsum_lag_180', 'demand_cumsum_lag_28', 'demand_cumsum_lag_364', 'demand_cumsum_lag_90', 'demand_lag_28_roll_kurt_30', 'demand_lag_28_roll_mean_180', 'demand_lag_28_roll_mean_30', 'demand_lag_28_roll_mean_60', 'demand_lag_28_roll_mean_7', 'demand_lag_28_roll_mean_90', 'demand_lag_28_roll_skew_30', 'demand_lag_28_roll_std_180', 'demand_lag_28_roll_std_30', 'demand_lag_28_roll_std_60', 'demand_lag_28_roll_std_7', 'demand_lag_28_roll_std_90', 'demand_lag_28_x', 'demand_lag_28_y', 'demand_lag_29', 'demand_lag_30', 'demand_lag_35', 'demand_lag_42', 'demand_lag_49', 'dept_id', 'deviation_sales_day_store_dept', 'devine_demand_day_store_dept_lag28', 'diff_ave_demand_day_store_dept_lag28', 'diff_ave_sales_day_store_dept', 'diff_lag_ave_sales_day_store_dept1', 'event_name_1', 'event_name_2', 'event_type_1', 'event_type_2', 'is_weekend', 'is_year_end', 'is_year_start', 'item_id', 'lag_deviation_sales_day_store_dept', 'month', 'price_change_t1', 'price_change_t365', 'price_rolling_std_t30', 'price_rolling_std_t7', 'sell_price', 'snap_CA', 'snap_TX', 'snap_WI', 'state_id', 'store_id', 'year']\n",
      "########################\n"
     ]
    }
   ],
   "source": [
    "\n",
    "use_top_importance = False\n",
    "num_features = 50\n",
    "\n",
    "\n",
    "\n",
    "########################\n",
    "print('########################')\n",
    "# read_transfomed\n",
    "print('read_transfomed_data')\n",
    "t0 = time.time()\n",
    "df_all = pd.read_pickle('./35093990_33386550_melt_over0sellprice.pkl')\n",
    "df_all = reduce_mem_usage(df_all)\n",
    "print(df_all.shape)\n",
    "df_all = df_all.query('sell_price > 0')\n",
    "print(df_all.shape)\n",
    "t1 = time.time()\n",
    "print('read_transfomed_data:{0}'.format(t1-t0) + '[sec]')\n",
    "print('########################')\n",
    "########################\n",
    "\n",
    "########################\n",
    "print('########################')\n",
    "print('merge_features...')\n",
    "print('before_merged_shape:{}'.format(df_all.shape))\n",
    "t0_all = time.time()\n",
    "\n",
    "f_paths = [\n",
    "    './feature/shop/f_devine_ave_lag28demand_day_store_dept_no_roll.pkl',\n",
    "    './feature/shop/f_diff_ave_lag28demand_day_store_dept_no_rolling.pkl',\n",
    "    './feature/zero_demand/f_id_zero_demand.pkl',\n",
    "    './feature/cumsum/f_id_cumsum_demand.pkl',\n",
    "    './feature/shop/f_diff_ave_sales_day_store_dept_std.pkl',\n",
    "    './feature/lag_demand/f_id_lag_demand_4weekdays_stat.pkl',\n",
    "    './feature/shop/f_diff_ave_sales_day_store_dept.pkl',\n",
    "    './feature/lag_demand/f_id_lag_demand.pkl',\n",
    "    './feature/lag_sales/f_id_lag_sales.pkl'\n",
    "]\n",
    "\n",
    "for f_path in f_paths:\n",
    "    t0 = time.time()\n",
    "    print(f_path)\n",
    "    df_f = pd.read_pickle(f_path)\n",
    "    reduce_mem_usage(df_f)\n",
    "    print('feature_shape:{}'.format(df_f.shape))\n",
    "    df_all = pd.merge(df_all, df_f, on=['id', 'date'], how='left')\n",
    "    del df_f\n",
    "    gc.collect()\n",
    "    print('merged_shape:{}'.format(df_all.shape))\n",
    "    t1 = time.time()\n",
    "    print('merged_time:{0}'.format(t1-t0) + '[sec]')\n",
    "\n",
    "print('all_merge_done')\n",
    "t1 = time.time()\n",
    "print('all_merged_time:{0}'.format(t1-t0_all) + '[sec]')\n",
    "print('########################')\n",
    "########################\n",
    "\n",
    "\n",
    "########################\n",
    "print('########################')\n",
    "print('date_features...')\n",
    "print('before_date_shape:{}'.format(df_all.shape))\n",
    "\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])\n",
    "# 対象\n",
    "attrs = [\"year\", \"month\", \"dayofweek\", \"is_year_end\", \"is_year_start\"]\n",
    "# is_year_end, is_year_srart\n",
    "\n",
    "for attr in attrs:\n",
    "    dtype = np.int16 if attr == \"year\" else np.int8\n",
    "    print(attr)\n",
    "    df_all[attr] = getattr(df_all['date'].dt, attr).astype(dtype)\n",
    "df_all[\"is_weekend\"] = df_all[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n",
    "print('add_date_shape:{}'.format(df_all.shape))\n",
    "t1 = time.time()\n",
    "print('date_feature:{0}'.format(t1-t0) + '[sec]')\n",
    "print('########################')\n",
    "########################\n",
    "\n",
    "\n",
    "########################\n",
    "# setting_feature\n",
    "print('df_all_col')\n",
    "print(df_all.columns)\n",
    "target_col = 'demand'\n",
    "useless_cols = ['id', 'part',\n",
    "                'date', 'wm_yr_wk', 'quarter', 'week', 'day',\n",
    "                'is_quarter_end', 'is_quarter_start',\n",
    "                'is_month_end', 'is_month_start',\n",
    "                'release',\n",
    "                # \"is_year_end\", \"is_year_start\"\n",
    "                ]\n",
    "# use: year, month, dayofweek, is_year_end, is_year_start, is_weekend\n",
    "x_features = [col for col in df_all.columns if col not in list(useless_cols + [target_col])]\n",
    "\n",
    "if use_top_importance:\n",
    "    csv_path = os.path.join(result_dir, 'importances.csv')\n",
    "    df_importance = pd.read_csv(csv_path)\n",
    "    df_importance.sort_values('gain', ascending=False, inplace=True)\n",
    "    x_features = list(df_importance.head(num_features)['feature'])\n",
    "    result_dir = os.path.join(result_dir, 'use_top_{}_importance_features'.format(num_features))\n",
    "    os.makedirs(result_dir, exist_ok=True)\n",
    "    print(result_dir)\n",
    "\n",
    "\n",
    "use_features = x_features + [target_col] + ['id', 'date']\n",
    "x_features = list(set(x_features))\n",
    "\n",
    "print('len_x_features:{}'.format(len(x_features)))\n",
    "# sort\n",
    "x_features = sorted(x_features)\n",
    "print(x_features)\n",
    "print('########################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merticの導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:36:34.573316Z",
     "start_time": "2020-05-06T14:36:34.569443Z"
    }
   },
   "outputs": [],
   "source": [
    "from metric import WRMSSEEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:36:42.688772Z",
     "start_time": "2020-05-06T14:36:35.167222Z"
    }
   },
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(\"../../input/calendar.csv\")\n",
    "sales_train_validation = pd.read_csv(\"../../input/sales_train_validation.csv\")\n",
    "sell_prices = pd.read_csv( \"../../input/sell_prices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:45.172139Z",
     "start_time": "2020-05-06T14:37:14.415265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755b483c578c4e68bcc2fb622bb7f3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_fold_df = sales_train_validation.copy() #weightの期間を変更\n",
    "valid_fold_df = sales_train_validation.iloc[:, -28:].copy()\n",
    "\n",
    "#インスタンスの作成\n",
    "evaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, sell_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:45.184281Z",
     "start_time": "2020-05-06T14:37:45.173531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        item_id    dept_id   cat_id store_id  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES     CA_1   \n",
       "1  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES     CA_1   \n",
       "2  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES     CA_1   \n",
       "3  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES     CA_1   \n",
       "4  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES     CA_1   \n",
       "\n",
       "  state_id  \n",
       "0       CA  \n",
       "1       CA  \n",
       "2       CA  \n",
       "3       CA  \n",
       "4       CA  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sales_train_validation.iloc[:,:6].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:45.797306Z",
     "start_time": "2020-05-06T14:37:45.185870Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# df_id = sales_train_validation.iloc[:,:6].copy()\n",
    "df_id = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']]\n",
    "df_id[\"all_id\"] = 0\n",
    "for i in range(12):\n",
    "    df_weight = getattr(evaluator,\"lv{}_weight\".format(i+1)).reset_index() #evaluatorからweightを取得\n",
    "    df_weight = df_weight.rename(columns = {0:\"weight\"})\n",
    "    \n",
    "    #各groupのsample数を計算\n",
    "    group_keys = df_weight.columns[:-1].tolist()\n",
    "    df_count = df_id.groupby(group_keys).count().id.reset_index()\n",
    "    df_count = df_count.rename(columns = {\"id\":\"n_sample\"})\n",
    "    \n",
    "    df_weight = pd.merge(df_weight,df_count,how=\"left\",on=(group_keys))\n",
    "    assert df_weight.isnull().sum().sum()==0,\"nullがあります\"\n",
    "    \n",
    "    df_weight[\"weight_{}\".format(i+1)] = df_weight[\"weight\"] / df_weight[\"n_sample\"] #weightの計算\n",
    "    df_id = pd.merge(df_id,df_weight.drop([\"weight\",\"n_sample\"],axis=1),how=\"left\",on=(group_keys)) #df_idにマージ\n",
    "\n",
    "#scaleの計算\n",
    "df_scale = getattr(evaluator, f'lv{12}_train_df').reset_index()[[\"item_id\",\"store_id\",\"scale\"]]\n",
    "df_id = pd.merge(df_id,df_scale,how=\"left\",on=([\"item_id\",\"store_id\"]))\n",
    "\n",
    "#最終weightの計算\n",
    "# df_id[\"ajust_weight\"] = df_id.iloc[:,7:19].mean(axis=1)*30490/np.sqrt(df_id[\"scale\"]) #RMSSEはscaleにrootがあるので、rootをかけて補正\n",
    "df_id[\"ajust_weight\"] = df_id.iloc[:,7:19].mean(axis=1)*30490 #RMSSEはscaleにrootがあるので、rootをかけて補正\n",
    "\n",
    "#必要な列だけ抽出\n",
    "df_weight = pd.DataFrame()\n",
    "df_weight[\"id\"] = df_id[\"id\"]\n",
    "df_weight[\"ajust_weight\"] = df_id[\"ajust_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:45.805258Z",
     "start_time": "2020-05-06T14:37:45.798785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ajust_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>1.108836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.770340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.808712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>1.297677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.935565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id  ajust_weight\n",
       "0  HOBBIES_1_001_CA_1_validation      1.108836\n",
       "1  HOBBIES_1_002_CA_1_validation      0.770340\n",
       "2  HOBBIES_1_003_CA_1_validation      0.808712\n",
       "3  HOBBIES_1_004_CA_1_validation      1.297677\n",
       "4  HOBBIES_1_005_CA_1_validation      0.935565"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weight.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:45.809693Z",
     "start_time": "2020-05-06T14:37:45.806656Z"
    }
   },
   "outputs": [],
   "source": [
    "tr_val_date = {\n",
    "    '1st': {\n",
    "        'train_end_date': '2016-02-28',\n",
    "        'val_end_date': '2016-03-27',\n",
    "        'train_end_date_num': 1857\n",
    "        \n",
    "    },\n",
    "    '2nd': {\n",
    "        'train_end_date': '2016-03-27',\n",
    "        'val_end_date': '2016-04-24',\n",
    "        'train_end_date_num': 1885\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:53.756033Z",
     "start_time": "2020-05-06T14:37:45.811171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "make_holdout\n",
      "rm_same_name_col\n",
      "(32782835, 68)\n",
      "(32782835, 68)\n",
      "sep...\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "print('########################')\n",
    "print('make_holdout')\n",
    "t0 = time.time()\n",
    "df_all = df_all[use_features]\n",
    "print('rm_same_name_col')\n",
    "print(df_all.shape)\n",
    "df_all = df_all.loc[:, ~df_all.columns.duplicated()]\n",
    "print(df_all.shape)\n",
    "\n",
    "print('sep...')\n",
    "df_test = df_all.query('date > \"2016-04-24\" and date <= \"2016-05-22\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:53.760544Z",
     "start_time": "2020-05-06T14:37:53.757468Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'metric': 'rmse',\n",
    "    'objective': 'poisson',\n",
    "    'n_jobs': -1,\n",
    "    'seed': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'alpha': 0.1,\n",
    "    'lambda': 0.1,\n",
    "    'bagging_fraction': 0.66,\n",
    "    'bagging_freq': 2,\n",
    "    'colsample_bytree': 0.77\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T14:37:53.768124Z",
     "start_time": "2020-05-06T14:37:53.762297Z"
    }
   },
   "outputs": [],
   "source": [
    "#元dfに対して予測して、wide_formatで返す関数\n",
    "# df_features：特徴量とtarget(true)を含むdf\n",
    "# df_keys：wide_formatに戻す際に必要となるkeysが入ったdf\n",
    "def pred_and_convert_wide(df_features, x_features, df_keys,model,df_scale=None,ajust_scale=False,scale_cols=None):\n",
    "    #dfの作成\n",
    "    pred_df = pd.DataFrame()\n",
    "    pred_df[\"demand\"] = model.predict(df_features[x_features])\n",
    "    pred_df['id'] = list(df_features['id'])\n",
    "    pred_df['date'] = list(df_features['date'])\n",
    "\n",
    "    if ajust_scale:\n",
    "        df_scale_tmp = df_scale.copy()\n",
    "        df_scale_tmp[\"id\"] = df_features['id']\n",
    "        pred_df = pd.merge(pred_df,df_scale_tmp,how=\"left\",on=\"id\")\n",
    "        pred_df[\"demand\"] = pred_df[\"demand\"] * np.sqrt(pred_df[\"scale\"])\n",
    "        pred_df = pred_df.drop(\"scale\",axis=1)\n",
    "    \n",
    "    print(pred_df.head())\n",
    "    #submission用に変換\n",
    "    pred_df = pd.pivot(pred_df, index = 'id', columns = 'date', values = 'demand').reset_index()\n",
    "    \n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-06T14:37:18.974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "va:2016-02-28-2016-03-27\n",
      "df_train:(29367955, 68)_df_val:(853720, 68)\n",
      "caluculate_weight...\n",
      "build_evaluater...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bc543161c84643ac07c43eff166169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train...\n",
      "train\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[50]\ttraining's rmse: 4.31675\tvalid_1's rmse: 3.86027\n",
      "[100]\ttraining's rmse: 4.11252\tvalid_1's rmse: 3.53859\n",
      "[150]\ttraining's rmse: 4.01552\tvalid_1's rmse: 3.49061\n",
      "[200]\ttraining's rmse: 3.88597\tvalid_1's rmse: 3.48799\n",
      "[250]\ttraining's rmse: 3.77017\tvalid_1's rmse: 3.4921\n",
      "[300]\ttraining's rmse: 3.68962\tvalid_1's rmse: 3.49776\n",
      "[350]\ttraining's rmse: 3.60826\tvalid_1's rmse: 3.4907\n",
      "Early stopping, best iteration is:\n",
      "[185]\ttraining's rmse: 3.92969\tvalid_1's rmse: 3.47338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     demand                             id       date\n",
      "0  0.715218  HOBBIES_1_001_CA_1_validation 2016-02-29\n",
      "1  0.394235  HOBBIES_1_002_CA_1_validation 2016-02-29\n",
      "2  0.393770  HOBBIES_1_003_CA_1_validation 2016-02-29\n",
      "3  1.904144  HOBBIES_1_004_CA_1_validation 2016-02-29\n",
      "4  0.867271  HOBBIES_1_005_CA_1_validation 2016-02-29\n",
      "WRMSSE： 0.7402\n",
      "va:2016-03-27-2016-04-24\n",
      "df_train:(30221675, 68)_df_val:(853720, 68)\n",
      "caluculate_weight...\n",
      "build_evaluater...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0656a575b5e4381903bd292c2896e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train...\n",
      "train\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "imp_df_list = []\n",
    "evaluator_list = []\n",
    "wrmsse_score_list = []\n",
    "valid_true_df_list = []\n",
    "valid_pred_df_list = []\n",
    "eval_result_list = []\n",
    "\n",
    "\n",
    "for num in ['1st', '2nd']:\n",
    "    train_end_date = tr_val_date[num]['train_end_date']\n",
    "    val_end_date = tr_val_date[num]['val_end_date']\n",
    "    print(f'va:{train_end_date}-{val_end_date}')\n",
    "    df_train = df_all.query('date <= @train_end_date')\n",
    "    df_val = df_all.query('date > @train_end_date and date <= @val_end_date')\n",
    "    print('df_train:{}_df_val:{}'.format(df_train.shape, df_val.shape, ))\n",
    "    #weightを計算\n",
    "    print('caluculate_weight...')\n",
    "    ajust_weight_train = pd.merge(df_train['id'], df_weight, how=\"left\", on=\"id\")['ajust_weight']\n",
    "    ajust_weight_val = pd.merge(df_val['id'], df_weight, how=\"left\", on=\"id\")['ajust_weight']\n",
    "    \n",
    "    if num == '1st':\n",
    "        train_fold_df = sales_train_validation.copy() #weightの期間を変更\n",
    "        valid_fold_df = sales_train_validation.iloc[:, -56:-28].copy()\n",
    "    else:\n",
    "        train_fold_df = sales_train_validation.copy() #weightの期間を変更\n",
    "        valid_fold_df = sales_train_validation.iloc[:, -28:].copy()\n",
    "    valid_true_df_list.append(valid_fold_df)\n",
    "    \n",
    "    #インスタンスの作成\n",
    "    print('build_evaluater...')\n",
    "    evaluator = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, sell_prices)\n",
    "    evaluator_list.append(evaluator)\n",
    "    \n",
    "    print('train...')\n",
    "    train_set = lgb.Dataset(df_train[x_features], df_train[target_col], weight=ajust_weight_train)\n",
    "    val_set = lgb.Dataset(df_val[x_features], df_val[target_col], weight=ajust_weight_val)\n",
    "    print('train')\n",
    "    evals_result = {}\n",
    "    model = lgb.train(\n",
    "            params,\n",
    "            train_set,\n",
    "            num_boost_round=5000,\n",
    "            early_stopping_rounds=200,\n",
    "            valid_sets=[train_set, val_set],\n",
    "    #         feval=evaluator.feval,\n",
    "            verbose_eval=50)\n",
    "    # 書き出し\n",
    "    model_path = os.path.join(result_dir, f'model_{num}.lgb')\n",
    "    model.save_model(model_path)\n",
    "    \n",
    "    # 予測\n",
    "    y_pred = model.predict(df_test[x_features], num_iteration=model.best_iteration)\n",
    "    df_test['demand'] += y_pred / 2.\n",
    "    \n",
    "    # 重要度\n",
    "    importances = pd.DataFrame()\n",
    "    importances['feature'] = x_features\n",
    "    importances['gain'] = model.feature_importance()\n",
    "    def save_importances(importances_: pd.DataFrame):\n",
    "        csv_path = os.path.join(result_dir, f'{num}_importances.csv')\n",
    "        importances_.to_csv(csv_path, index=False)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        sns.barplot(\n",
    "            x='gain',\n",
    "            y='feature',\n",
    "            data=importances_.sort_values('gain', ascending=False)[:50])\n",
    "        png_path = os.path.join(result_dir, f'{num}_importances.png')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(png_path)\n",
    "\n",
    "    save_importances(importances)\n",
    "    \n",
    "    model_list.append(model)\n",
    "    imp_df_list.append(importances)\n",
    "    eval_result_list.append(evals_result)\n",
    "    \n",
    "    #WRMSSEの算出\n",
    "    id_columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    X_val_wide = pred_and_convert_wide(df_val ,x_features, df_val[[\"id\",\"date\"]].copy(), model)\n",
    "    X_val_wide.columns = ['id'] + ['d_' + str(i + 1) for i in range(tr_val_date[num]['train_end_date_num'], tr_val_date[num]['train_end_date_num']+28)]\n",
    "    valid_preds = pd.merge(train_fold_df[id_columns].copy(), X_val_wide, how=\"left\",on=\"id\")\n",
    "    valid_pred_df_list.append(valid_preds)\n",
    "    #スコアの算出\n",
    "    wrmsse_score = evaluator.score(valid_preds.iloc[:,6:]) #id列は削除して渡す\n",
    "    wrmsse_score_list.append(wrmsse_score)\n",
    "    print(\"WRMSSE：\",round(wrmsse_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-06T14:37:19.321Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_timeseries_summary(valid_pred_df_list,valid_true_df_list,sales_train_validation,plot_col):\n",
    "\n",
    "    sales_train_validation[\"all\"] = 0\n",
    "\n",
    "    #predsの作成\n",
    "    id_cols = [\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"all\"] #allを追加\n",
    "    drop_cols = [x for x in id_cols if x not in plot_col] #plot_colにない値だけを返す\n",
    "\n",
    "    preds_list = []\n",
    "    truth_list = []\n",
    "\n",
    "    for preds, truth in zip(valid_pred_df_list,valid_true_df_list):\n",
    "        #予測データ\n",
    "        preds[\"all\"] = 0\n",
    "        X_val_preds_T = preds.drop(drop_cols,axis=1).groupby(plot_col).sum().T.reset_index().rename(columns={\"index\":\"day\"})\n",
    "        preds_list.append(X_val_preds_T)\n",
    "\n",
    "        #正解データ\n",
    "        X_val_truth_T = pd.concat([sales_train_validation[plot_col],truth],axis=1).groupby(plot_col).sum().T.reset_index().rename(columns={\"index\":\"day\"})\n",
    "        truth_list.append(X_val_truth_T)\n",
    "\n",
    "\n",
    "    if type(plot_col) == list:\n",
    "        plot_col = plot_col[0]\n",
    "\n",
    "    #plot\n",
    "    n_plot = len(X_val_preds_T.columns)-1\n",
    "    n_row = math.ceil(n_plot/3)\n",
    "    plt.figure(figsize=[30,n_row*7])\n",
    "\n",
    "    if plot_col == \"all\":\n",
    "        for i in range(2):\n",
    "            plt.subplot(n_row,2,i+1)\n",
    "            for j,col in enumerate(preds_list[i].columns[1:]):\n",
    "                sns.lineplot(x=\"day\",y=col,data=truth_list[i])\n",
    "                sns.lineplot(x=\"day\",y=col,data=preds_list[i])\n",
    "                plt.ylabel(\"sales\")\n",
    "                plt.xticks(valid_true_df_list[i].columns.tolist()[::7])\n",
    "                plt.title(\"all sales in validation{}\".format(i),fontsize=18)\n",
    "                plt.legend([\"truth\",\"preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-06T14:37:19.498Z"
    }
   },
   "outputs": [],
   "source": [
    "#予測値の可視化（Validation期間）\n",
    "def plot_timeseries(X_val_preds,valid_fold_df,sales_train_validation,plot_col,n_val):\n",
    "\n",
    "    sales_train_validation[\"all\"] = 0\n",
    "    X_val_preds[\"all\"] = 0\n",
    "\n",
    "    #predsの作成\n",
    "    id_cols = [\"id\",\"item_id\",\"dept_id\",\"cat_id\",\"store_id\",\"state_id\",\"all\"] #allを追加\n",
    "    drop_cols = [x for x in id_cols if x not in plot_col] #plot_colにない値だけを返す\n",
    "    #予測データ\n",
    "    X_val_preds_T = X_val_preds.drop(drop_cols,axis=1).groupby(plot_col).sum().T.reset_index().rename(columns={\"index\":\"day\"})\n",
    "    #正解データ\n",
    "    X_val_truth_T = pd.concat([sales_train_validation[plot_col],valid_fold_df],axis=1).groupby(plot_col).sum().T.reset_index().rename(columns={\"index\":\"day\"})\n",
    "\n",
    "    if type(plot_col) == list:\n",
    "        plot_col = plot_col[0]\n",
    "\n",
    "    #plot\n",
    "    n_plot = len(X_val_preds_T.columns)-1\n",
    "    n_row = math.ceil(n_plot/3)\n",
    "    plt.figure(figsize=[30,n_row*7])\n",
    "\n",
    "    if plot_col == \"all\":\n",
    "        for i,col in enumerate(X_val_preds_T.columns[1:]):\n",
    "            sns.lineplot(x=\"day\",y=col,data=X_val_truth_T)\n",
    "            sns.lineplot(x=\"day\",y=col,data=X_val_preds_T)\n",
    "            plt.ylabel(\"sales\")\n",
    "            plt.xticks(valid_fold_df.columns.tolist()[::7])\n",
    "            plt.title(\"all sales in validation{}\".format(n_val),fontsize=18)\n",
    "            plt.legend([\"truth\",\"preds\"])\n",
    "    \n",
    "    else:\n",
    "        for i,col in enumerate(X_val_preds_T.columns[1:]):\n",
    "            plt.subplot(n_row,3,i+1) #横に3つずつプロットする\n",
    "            # plt.subplots_adjust(wspace=0.1,top=0.5)\n",
    "            plt.suptitle(\"{} validation{}\".format(plot_col,n_val),fontsize=24,va=\"bottom\",y=0.9) #super titleの設定\n",
    "            sns.lineplot(x=\"day\",y=col,data=X_val_truth_T)\n",
    "            sns.lineplot(x=\"day\",y=col,data=X_val_preds_T)\n",
    "            plt.ylabel(\"sales\")\n",
    "            plt.xticks(valid_fold_df.columns.tolist()[::7])\n",
    "            plt.title(\"{} sales in validation{}\".format(col,n_val),fontsize=18)\n",
    "            plt.legend([\"truth\",\"preds\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-06T14:37:19.677Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot_col_list = [\"all\",\"store_id\",\"dept_id\"]\n",
    "plot_col_list = [\"store_id\",\"dept_id\"]\n",
    "plot_timeseries_summary(valid_pred_df_list,valid_true_df_list,sales_train_validation,plot_col=\"all\")# allだけ\n",
    "for i in range(2):\n",
    "    for plot_col in plot_col_list:\n",
    "        plot_timeseries(valid_preds,valid_fold_df,sales_train_validation,plot_col,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 書き出し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-06T14:37:20.210Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(test, submission, csv_path):\n",
    "    predictions = test[['id', 'date', 'demand']]\n",
    "    predictions = pd.pivot(predictions, index='id', columns='date', values='demand').reset_index()\n",
    "    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n",
    "\n",
    "    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row]\n",
    "    evaluation = submission[submission['id'].isin(evaluation_rows)]\n",
    "\n",
    "    validation = submission[['id']].merge(predictions, on='id')\n",
    "    final = pd.concat([validation, evaluation])\n",
    "    print(final.head())\n",
    "    print(final.shape)\n",
    "    final.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-06T14:37:20.445Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "csv_path = os.path.join(result_dir, 'sub.csv')\n",
    "predict(df_test, submission, csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
